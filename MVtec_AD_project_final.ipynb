{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VoTboyOZWXNxAbXNN1I9jwZHWZ1IhWkQ",
      "authorship_tag": "ABX9TyPDu5t/t+fCGp34skJtEx+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayver/Mvtec-AD-project/blob/main/MVtec_AD_project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE7HAW7-P-C_"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras import Input, Model, Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from random import randrange"
      ],
      "metadata": {
        "id": "RDJOSyDnQJhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, path = \"/content/drive/MyDrive/mvtec_anomaly_detection/screw/\",\n",
        "                 batch_size = 4,\n",
        "                 input_size=(224, 224),\n",
        "                 shuffle=True, seed = None, subset = 'training'):\n",
        "\n",
        "        self.image_data_generator = ImageDataGenerator(rescale=1. / 255, data_format='channels_last',\n",
        "            #zoom_range = 0.1,\n",
        "            #width_shift_range = 0.05,\n",
        "            #height_shift_range = 0.05,\n",
        "            #brightness_range=(0.95,1.05)\n",
        "        )\n",
        "\n",
        "        if seed is None:\n",
        "            random.randint(0, 2**32)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.X_paths = [os.fsdecode(file) for file in os.scandir(f\"{path}/train/good\")]\n",
        "        self.X_train, self.X_val = train_test_split(self.X_paths, test_size=0.2, random_state = seed, shuffle = True)\n",
        "\n",
        "        if subset == 'training':\n",
        "            self.X = self.X_train\n",
        "        elif subset == 'validation':\n",
        "            self.X = self.X_val\n",
        "\n",
        "        self.n = len(self.X)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data_x = []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            image = cv2.imread(self.X[self.batch_size * index + i])\n",
        "\n",
        "            image = self.image_data_generator.random_transform(image)\n",
        "\n",
        "            image = image / 255\n",
        "\n",
        "            image = tf.image.resize(image, self.input_size)\n",
        "            data_x.append(image)\n",
        "\n",
        "        data_x = np.array(data_x)\n",
        "\n",
        "        return data_x, data_x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size"
      ],
      "metadata": {
        "id": "5HCuGea2QLZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = (224,224)\n",
        "seed = random.randint(0, 2**32)\n",
        "train_datagen = CustomDataGen(input_size = INPUT_SIZE, batch_size = 4, seed = seed)\n",
        "validation_datagen = CustomDataGen(input_size = INPUT_SIZE, subset = \"validation\", seed = seed)"
      ],
      "metadata": {
        "id": "c8wMvbVAQNy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check we dont have the same images in train and validation sets\n",
        "any([value in train_datagen.X for value in validation_datagen.X])"
      ],
      "metadata": {
        "id": "3qnXPtVNQQih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = train_datagen[0]\n",
        "io.imshow(x[0])\n",
        "io.show()"
      ],
      "metadata": {
        "id": "BUb-qgEmQUEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_model_filepath = \"../input/anomalysegmentation-weights/anomaly-segmentation-model_vgg19.h5\"\n",
        "output_model_filepath = \"./anomaly-segmentation-model.h5\"\n",
        "#model class\n",
        "#Implementation of DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation https://arxiv.org/abs/2012.07122\n",
        "class AnomalySegmentator(tf.keras.Model):\n",
        "    def __init__(self, init_layer = 0, end_layer = None):\n",
        "        super(AnomalySegmentator, self).__init__()\n",
        "        #self.L2_weight = 1e-6\n",
        "        self.init_layer = init_layer\n",
        "        self.end_layer = end_layer\n",
        "\n",
        "    def build_autoencoder(self, c0, cd):\n",
        "        self.autoencoder = Sequential([\n",
        "            layers.InputLayer((self.map_shape[0]//4, self.map_shape[1]//4, c0)),\n",
        "            layers.Conv2D((c0 + cd) // 2,(1,1), padding='same', activation = tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "            layers.Conv2D(2*cd,(1,1), padding='same', activation = tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "            layers.Conv2D(cd,(1,1), padding='same'),\n",
        "            layers.Conv2D(2*cd,(1,1), padding='same', activation = tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "            layers.Conv2D((c0 + cd) // 2,(1,1), padding='same', activation = tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
        "            layers.Conv2D(c0,(1,1), padding='same')\n",
        "\n",
        "        ])\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.vgg = VGG19(include_top = False, weights = 'imagenet', input_shape=input_shape[1:])\n",
        "        self.features_list = [layer.output for layer in self.vgg.layers if 'conv' in layer.name][self.init_layer:self.end_layer]\n",
        "\n",
        "        self.feature_extractor = Model(inputs = self.vgg.input,\n",
        "                                       outputs = self.features_list)\n",
        "        self.feature_extractor.trainable = False\n",
        "\n",
        "        self.threshold = tf.Variable(0, trainable = False, dtype = tf.float32)\n",
        "\n",
        "        self.map_shape = self.features_list[0].shape[1:-1]\n",
        "\n",
        "        self.average_pooling = layers.AveragePooling2D(pool_size=(4, 4), strides=(4,4))\n",
        "\n",
        "\n",
        "\n",
        "        self.c0 = sum([feature.shape[-1] for feature in self.features_list])\n",
        "        self.cd = 40\n",
        "        self.build_autoencoder(self.c0, self.cd)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features = self.feature_extractor(inputs)\n",
        "        resized_features = [tf.image.resize(feature, self.map_shape) for feature in features]\n",
        "        resized_features = tf.concat(resized_features, axis = -1)\n",
        "\n",
        "        resized_features = self.average_pooling(resized_features)\n",
        "\n",
        "        autoencoder_output = self.autoencoder(resized_features)\n",
        "        return tf.reduce_sum((autoencoder_output - resized_features)**2, axis = -1)\n",
        "\n",
        "    def reconstruction_loss(self):\n",
        "        @tf.function\n",
        "        def _loss(y_true, y_pred):\n",
        "            loss = tf.reduce_mean(y_pred, axis = (1,2)) / (tf.cast(tf.shape(y_pred)[0], tf.float32) * self.c0)\n",
        "            return loss\n",
        "\n",
        "        return _loss\n",
        "\n",
        "    def compute_threshold(self, data_loader, fpr = 0.05):\n",
        "      error = []\n",
        "      for i in tqdm(range(len(data_loader))):\n",
        "        x, y = data_loader[i]\n",
        "        error.append(self(x))\n",
        "\n",
        "      threshold = np.percentile(error, 100 - fpr)\n",
        "      self.threshold = tf.Variable(threshold, trainable = False, dtype = tf.float32)\n",
        "\n",
        "    def compute_pca(self, data_loader):\n",
        "        extraction_per_sample = 20\n",
        "\n",
        "        extractions = []\n",
        "        for i in tqdm(range(len(data_loader))):\n",
        "            x, _ = data_loader[i]\n",
        "\n",
        "            features = self.feature_extractor(x)\n",
        "            resized_features = [tf.image.resize(feature, self.map_shape) for feature in features]\n",
        "            resized_features = tf.concat(resized_features, axis = -1)\n",
        "\n",
        "            resized_features = self.average_pooling(resized_features)\n",
        "\n",
        "            for feature in resized_features:\n",
        "\n",
        "                for _ in range(extraction_per_sample):\n",
        "\n",
        "                    row, col = randrange(feature.shape[0]), randrange(feature.shape[1])\n",
        "                    extraction = feature[row, col]\n",
        "                    extractions.append(extraction)\n",
        "\n",
        "        extractions = np.array(extractions)\n",
        "        print(f\"Extractions Shape: {extractions.shape}\")\n",
        "        pca = PCA(0.9, svd_solver = \"full\")\n",
        "        pca.fit(extractions)\n",
        "        self.cd = pca.n_components_\n",
        "        self.build_autoencoder(self.c0, self.cd)\n",
        "        print(f\"Components with explainable variance 0.9 -> {self.cd}\")\n",
        "\n",
        "as_model = AnomalySegmentator()\n",
        "as_model.compile(Adam(1e-4), loss = as_model.reconstruction_loss())"
      ],
      "metadata": {
        "id": "Z00Ne8pAQYRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "as_model.build((None, *INPUT_SIZE,3))\n",
        "as_model.compute_pca(train_datagen)"
      ],
      "metadata": {
        "id": "phvjmdBDQZxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#True if we want to train the model\n",
        "if True:\n",
        "    # Training the model\n",
        "    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor='val_loss', factor=0.5, patience=5, verbose = 1\n",
        "    )\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
        "                                          patience=15)  # Early stopping (stops training when validation doesn't improve for {patience} epochs)\n",
        "    save_best = tf.keras.callbacks.ModelCheckpoint(output_model_filepath, monitor='val_loss', save_best_only=True,\n",
        "                                                mode='min', save_weights_only = True)  # Saves the best version of the model to disk (as measured on the validation data set)\n",
        "\n",
        "    history = as_model.fit(train_datagen,\n",
        "        epochs=500,\n",
        "        validation_data=validation_datagen,\n",
        "        shuffle=True,\n",
        "        callbacks=[es, save_best, plateau])\n",
        "    #Training history\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    as_model.load_weights(output_model_filepath)\n",
        "    as_model.compute_threshold(train_datagen)\n",
        "    as_model.save_weights(output_model_filepath)\n",
        "else:\n",
        "\n",
        "    as_model.build((None, *INPUT_SIZE,3))\n",
        "    as_model.load_weights(input_model_filepath)\n",
        "    as_model.compute_threshold(train_datagen)\n",
        "    as_model.summary()\n",
        "    as_model.autoencoder.summary()"
      ],
      "metadata": {
        "id": "tRWf2GSaQaSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['good', 'manipulated_front', 'scratch_head', 'scratch_neck', 'thread_side', 'thread_top'] #only 0 index is GOOD\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255, data_format='channels_last')\n",
        "test_generator = test_datagen.flow_from_directory(\"/content/drive/MyDrive/mvtec_anomaly_detection/screw/test\",\n",
        "                                                  target_size = INPUT_SIZE,\n",
        "                                                  batch_size = 1,\n",
        "                                                  class_mode = 'sparse')"
      ],
      "metadata": {
        "id": "dGEz-BUXQdah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    x_batch, y_batch = next(test_generator)\n",
        "    hotmaps = as_model(x_batch)\n",
        "    for x, y, hotmap in zip(x_batch, y_batch, hotmaps):\n",
        "\n",
        "        prediction = np.any(hotmap > as_model.threshold)\n",
        "\n",
        "        hotmap = resize(hotmap, x.shape[:-1], anti_aliasing = True)\n",
        "        mask = np.where(hotmap > as_model.threshold, 1, 0)\n",
        "\n",
        "\n",
        "        f, axarr = plt.subplots(1,2, figsize=(15,15))\n",
        "        axarr[0].imshow(x)\n",
        "        axarr[1].imshow(mask)\n",
        "        axarr[1].imshow(x, alpha = 0.75)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Threshold: {as_model.threshold.numpy()} MaxValue: {hotmap.max()}\")\n",
        "        print(f\"GT: {classes[int(y)]}, anomaly detected: {prediction}\")\n",
        ""
      ],
      "metadata": {
        "id": "-COeQh8BQg6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}